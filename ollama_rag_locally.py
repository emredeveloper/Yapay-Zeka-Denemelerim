import streamlit as st
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
import ollama
import tempfile
import os
import matplotlib.pyplot as plt

# Streamlit arayÃ¼zÃ¼ baÅŸlÄ±ÄŸÄ±
st.title("ğŸ“„ RAG Sistemi ile PDF Soru-Cevap")
st.markdown("""
Bu uygulama, yÃ¼klediÄŸiniz bir PDF dosyasÄ±nÄ± iÅŸler ve sorularÄ±nÄ±za yanÄ±t verir.
""")

# Sidebar iÃ§in ince ayarlar
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    with st.expander("Metin ParÃ§alama AyarlarÄ±"):
        chunk_size = st.slider("Metin ParÃ§a Boyutu (Chunk Size)", 500, 2000, 1000)
        chunk_overlap = st.slider("Metin ParÃ§a Ã–rtÃ¼ÅŸmesi (Chunk Overlap)", 0, 500, 200)
    with st.expander("Model AyarlarÄ±"):
        model_name = st.selectbox("Model", ["deepseek-r1:1.5b", "llama2", "mistral"])
        temperature = st.slider("Model SÄ±caklÄ±ÄŸÄ± (Temperature)", 0.1, 1.0, 0.7)

# PDF yÃ¼kleme ve iÅŸleme fonksiyonu
def load_and_process_pdf(uploaded_file, chunk_size, chunk_overlap):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    try:
        loader = PyMuPDFLoader(tmp_file_path)
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        texts = text_splitter.split_documents(documents)
        return texts, tmp_file_path
    except Exception as e:
        st.error(f"PDF yÃ¼klenirken bir hata oluÅŸtu: {e}")
        return None, None

# VektÃ¶r veritabanÄ± oluÅŸturma fonksiyonu
def create_vector_store(texts, embeddings):
    try:
        vectorstore = Chroma.from_documents(texts, embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"VektÃ¶r veritabanÄ± oluÅŸturulurken bir hata oluÅŸtu: {e}")
        return None

# Soruya yanÄ±t oluÅŸturma fonksiyonu
def generate_response(query, context, model, temperature):
    try:
        response = ollama.generate(
            model=model,
            prompt=f"Soru: {query}\n\nBaÄŸlam: {context}\n\nYanÄ±t:",
            options={"temperature": temperature}
        )
        return response['response']
    except Exception as e:
        st.error(f"YanÄ±t oluÅŸturulurken bir hata oluÅŸtu: {e}")
        return None

# GÃ¶rselleÅŸtirme fonksiyonu
def visualize_text_chunks(texts):
    chunk_sizes = [len(text.page_content) for text in texts]
    fig, ax = plt.subplots()
    ax.hist(chunk_sizes, bins=20, color='blue', alpha=0.7)
    ax.set_title("Metin ParÃ§alarÄ±nÄ±n Boyut DaÄŸÄ±lÄ±mÄ±")
    ax.set_xlabel("ParÃ§a Boyutu")
    ax.set_ylabel("Frekans")
    st.pyplot(fig)

# KullanÄ±cÄ±dan PDF dosyasÄ± yÃ¼klemesi istenir
uploaded_file = st.file_uploader("ğŸ“¤ LÃ¼tfen bir PDF dosyasÄ± yÃ¼kleyin", type="pdf")

if uploaded_file is not None:
    # PDF yÃ¼kleme ve iÅŸleme
    with st.spinner("PDF yÃ¼kleniyor ve iÅŸleniyor..."):
        texts, tmp_file_path = load_and_process_pdf(uploaded_file, chunk_size, chunk_overlap)

    if texts:
        st.success("âœ… PDF baÅŸarÄ±yla yÃ¼klendi ve iÅŸlendi!")
        st.write(f"Toplam {len(texts)} metin parÃ§asÄ± oluÅŸturuldu.")

        # Metin parÃ§alarÄ±nÄ± gÃ¶rselleÅŸtirme (isteÄŸe baÄŸlÄ±)
        with st.expander("Metin ParÃ§alarÄ±nÄ± GÃ¶rselleÅŸtir"):
            visualize_text_chunks(texts)

        # OllamaEmbeddings kullanÄ±larak metinler vektÃ¶rleÅŸtirilir
        embeddings = OllamaEmbeddings(model=model_name)

        # VektÃ¶r veritabanÄ± oluÅŸturulur
        with st.spinner("VektÃ¶r veritabanÄ± oluÅŸturuluyor..."):
            vectorstore = create_vector_store(texts, embeddings)

        if vectorstore:
            st.success("âœ… VektÃ¶r veritabanÄ± baÅŸarÄ±yla oluÅŸturuldu!")

            # KullanÄ±cÄ±dan soru alÄ±nÄ±r
            query = st.text_input("â“ LÃ¼tfen bir soru girin:")

            if query:
                with st.spinner("Soruya en uygun metin parÃ§alarÄ± aranÄ±yor..."):
                    # Soruya en uygun metin parÃ§alarÄ± bulunur
                    docs = vectorstore.similarity_search_with_score(query, k=3)
                    context = " ".join([doc[0].page_content for doc in docs])

                    # BaÄŸlam metnini gÃ¶ster (isteÄŸe baÄŸlÄ±)
                    with st.expander("BaÄŸlam Metnini GÃ¶ster"):
                        st.write(context)

                    # YanÄ±t oluÅŸturulur ve gÃ¶sterilir
                    with st.spinner("YanÄ±t oluÅŸturuluyor..."):
                        response = generate_response(query, context, model_name, temperature)
                        if response:
                            st.subheader("ğŸ“ YanÄ±t:")
                            st.write(response)

    # GeÃ§ici dosya silinir
    if tmp_file_path:
        os.remove(tmp_file_path)