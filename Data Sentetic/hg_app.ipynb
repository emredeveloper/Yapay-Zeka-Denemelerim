{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from huggingface_hub import login, HfApi\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authenticate with your Hugging Face account\n",
    "# Replace 'your_token_here' with your actual Hugging Face token\n",
    "login(token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\emreq\\Desktop\\Genel\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\emreq\\Desktop\\Genel\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\emreq\\Desktop\\Genel\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8cecf895ef46c683f1d90a8134c450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5fa5210faf455b907ae9bc944e8dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2262e01d4b7c47e48a5c79e7b2393bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/emredeveloper/synthetic-arxiv-abstracts-v1/commit/963581443d535083892ee87f10d1be3e6b218118', commit_message='Upload dataset', commit_description='', oid='963581443d535083892ee87f10d1be3e6b218118', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/emredeveloper/synthetic-arxiv-abstracts-v1', endpoint='https://huggingface.co', repo_type='dataset', repo_id='emredeveloper/synthetic-arxiv-abstracts-v1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load a pre-trained model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")  # Use GPU if available\n",
    "\n",
    "# Step 2: Define a function to generate synthetic data\n",
    "def generate_synthetic_data(prompt, num_samples=10, max_length=512):\n",
    "    synthetic_data = []\n",
    "    for _ in range(num_samples):\n",
    "        try:\n",
    "            # Generate the abstract (response)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True).to(model.device)\n",
    "            outputs = model.generate(**inputs, max_length=max_length, temperature=1.2, top_p=0.9)\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Generate a title for the abstract\n",
    "            title_prompt = f\"Generate a concise title for the following abstract: {response}\"\n",
    "            title_inputs = tokenizer(title_prompt, return_tensors=\"pt\", max_length=max_length, truncation=True).to(model.device)\n",
    "            title_outputs = model.generate(**title_inputs, max_length=50, temperature=0.7, top_p=0.9)\n",
    "            title = tokenizer.decode(title_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Append the data with prompt, response, and title\n",
    "            synthetic_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"title\": title\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {_}: {e}\")\n",
    "    return synthetic_data\n",
    "\n",
    "# Step 3: Generate synthetic data (e.g., research paper abstracts)\n",
    "prompt = \"Generate an arXiv abstract of an NLP research paper. Return just the abstract, no titles.\"\n",
    "synthetic_data = generate_synthetic_data(prompt, num_samples=10)\n",
    "\n",
    "# Step 4: Convert the synthetic data to a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(synthetic_data)\n",
    "\n",
    "# Step 5: Add dataset metadata (optional but recommended)\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "\n",
    "# Step 6: Push the dataset to the Hugging Face Hub\n",
    "repo_name = \"emredeveloper/synthetic-arxiv-abstracts-v1\"  # Use the correct repository name\n",
    "dataset.push_to_hub(repo_name, private=False)  # Set private=True if you want to keep it private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6783d345c1cd44e5acb6fd63b4c8f149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3516f43c67048e299a8aa7085bef9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a997a1d4284684884aff7cd38614be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/378 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved locally at ./deepseek-1.5B-local\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate and configure\n",
    "login(token='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')\n",
    "\n",
    "# Load model with optimized configuration\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    pad_token=\"<|endoftext|>\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "def generate_synthetic_data(prompt, num_samples=5):  # Örnek sayısını azalttık\n",
    "    synthetic_data = []\n",
    "    \n",
    "    # Optimize prompt processing\n",
    "    base_inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,  # Uzunluğu kısalttık\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generation parameters for speed\n",
    "    gen_kwargs = {\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"use_cache\": True  # Önbelleği etkinleştirdik\n",
    "    }\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            # Faster abstract generation\n",
    "            abstract_outputs = model.generate(\n",
    "                **base_inputs,\n",
    "                max_new_tokens=256,  # Token sayısını azalttık\n",
    "                min_new_tokens=150,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "            abstract = tokenizer.decode(abstract_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Optimized title generation\n",
    "            title_prompt = f\"Title for: {abstract[:500]}\"  # Girişi kısalttık\n",
    "            title_inputs = tokenizer(\n",
    "                title_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=256,\n",
    "                truncation=True\n",
    "            ).to(model.device)\n",
    "\n",
    "            title_outputs = model.generate(\n",
    "                **title_inputs,\n",
    "                max_new_tokens=30,  # Daha kısa başlıklar\n",
    "                min_new_tokens=10,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "            title = tokenizer.decode(title_outputs[0], skip_special_tokens=True).split(\":\")[-1].strip()\n",
    "\n",
    "            synthetic_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": abstract,\n",
    "                \"title\": title\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating sample {i}: {str(e)[:200]}\")\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate data with shorter prompt\n",
    "prompt = \"\"\"Generate NLP research abstract about ML text analysis. Include methods and results.\"\"\"\n",
    "\n",
    "synthetic_data = generate_synthetic_data(prompt, num_samples=5)  # Daha az örnek\n",
    "\n",
    "# Save and push\n",
    "dataset = Dataset.from_list(synthetic_data)\n",
    "dataset.push_to_hub(\"emredeveloper/synthetic-arxiv-abstracts-v1\", private=False)\n",
    "\n",
    "# Save locally\n",
    "save_path = \"./deepseek-1.5B-local\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model saved locally at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import random\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face kimlik doğrulaması (dataset push için)\n",
    "login(token='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')\n",
    "\n",
    "# Yerel model yolu\n",
    "local_model_path = \"./deepseek-1.5B-local\"\n",
    "\n",
    "# Model ve Tokenizer'ı yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    local_model_path,\n",
    "    pad_token=\"<|endoftext|>\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "def generate_natural_queries(num_queries=5):\n",
    "    \"\"\"Doğal görünen kullanıcı sorguları oluştur\"\"\"\n",
    "    templates = [\n",
    "        \"How to analyze {} using machine learning? Need methods and results.\",\n",
    "        \"Comparing {} techniques in NLP: which works better?\",\n",
    "        \"Best approach for {} analysis? Include practical examples.\",\n",
    "        \"Can you explain {} methods in simple terms? Need academic references.\",\n",
    "        \"What's the latest research on {}? Focus on real-world applications.\"\n",
    "    ]\n",
    "    \n",
    "    topics = [\n",
    "        \"text classification\", \n",
    "        \"topic modeling\", \n",
    "        \"sentiment analysis\",\n",
    "        \"fake news detection\",\n",
    "        \"document clustering\"\n",
    "    ]\n",
    "    \n",
    "    return [t.format(random.choice(topics)) for t in random.sample(templates, num_queries)]\n",
    "\n",
    "def generate_abstract(prompt):\n",
    "    \"\"\"Tek bir özet oluşturma fonksiyonu\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        min_new_tokens=150,\n",
    "        temperature=0.85,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def create_dataset(num_samples=5):\n",
    "    dataset = []\n",
    "    queries = generate_natural_queries(num_samples)\n",
    "    \n",
    "    for query in queries:\n",
    "        try:\n",
    "            abstract = generate_abstract(query)\n",
    "            \n",
    "            # Başlık üretimi\n",
    "            title_response = model.generate(\n",
    "                **tokenizer(\n",
    "                    f\"Generate a research title for: {abstract[:100]}\",\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(model.device),\n",
    "                max_new_tokens=40,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            title = tokenizer.decode(title_response[0], skip_special_tokens=True)\n",
    "            title = title.split(\"Title:\")[-1].strip()\n",
    "            \n",
    "            dataset.append({\n",
    "                \"user_query\": query,\n",
    "                \"generated_abstract\": abstract,\n",
    "                \"suggested_title\": title\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Hata oluştu: {str(e)[:200]}\")\n",
    "    \n",
    "    return Dataset.from_dict({k: [dic[k] for dic in dataset] for k in dataset[0]})\n",
    "\n",
    "# Veri setini oluştur ve kaydet\n",
    "dataset = create_dataset(5)\n",
    "dataset.push_to_hub(\"emredeveloper/synthetic-arxiv-abstracts-v1\")\n",
    "\n",
    "print(\"Örnek çıktı:\")\n",
    "print(dataset[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
